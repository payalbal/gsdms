---
title: 'extras for gbif-data_processing'
author: "Payal Bal"
---

## Run query to extract data and create a data table of 1000000 records at a time
# Issue - ORDER takeas too long to run, DROP OPTION
## Alternative: Run on the entire table and create new table in one go. 
length(seq(0, 572499021 ,1000000))

#temp <- rbindlist(foreach (i=seq(0,572499021,1000000)) %do% {...LIMIT 1000000...})

## Trial run with LIMIT 5 
temp <- rbindlist(foreach (i=seq(0,10,5)) %do% {
  data.table(dbGetQuery(con,"
                            SELECT *
                            FROM gbif.raw_data
                            WHERE kingdom = 'Animalia'
                            AND decimallatitude IS NOT NULL
                            AND decimallongitude IS NOT NULL
                            AND year BETWEEN '1950' AND '2017'
                            ORDER BY gbifid
                            LIMIT 5
                            OFFSET i;
                            "))
})


## Run query to extract data accroding to query and create a new data table 
temp <- data.table(dbGetQuery(con,"
                            SELECT *
                            FROM gbif.raw_data
                            WHERE kingdom = 'Animalia'
                            AND decimallatitude IS NOT NULL
                            AND decimallongitude IS NOT NULL
                            AND year BETWEEN '1950' AND '2017'
                            ORDER BY gbifid
                            LIMIT 5
                            OFFSET i;
                            "))
})


## Writing tabel to boab server - check with Casey
# dbWriteTable(con, c("dbnameonserver", "tablename"), value = tabletowrite, row.names=FALSE, overwrite=TRUE)
# dbWriteTable(con, c("gbif", "temp"), value = gbif, row.names=FALSE, overwrite=TRUE)